{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4515f-3194-4e54-9285-d9591e3621b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c4b36a-464d-437e-9731-833a1b5f845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_plot(type, csv_file, model_names, prompt_id, target_ratio, temperature):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    external_list = []\n",
    "    internal_list = []\n",
    "\n",
    "    num_models = len(model_names)\n",
    "    num_rows = math.ceil(num_models / 2)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, 2, figsize=(12, num_rows * 5))\n",
    "    axes = axes.flatten()  # Flatten to 1D array for easy iteration\n",
    "\n",
    "    # Loop through each model name\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        # Extract the true labels, predicted labels, and predicted probabilities\n",
    "        if prompt_id == 1:\n",
    "            if target_ratio is None:\n",
    "                if temperature is None:\n",
    "                    true_labels = df['propose_correct_answer']\n",
    "                    decisions = df[f'Decision ({model_name})']\n",
    "                    external_confidence = df[f'External Confidence ({model_name})']\n",
    "                    internal_confidence = df[f'Internal Confidence ({model_name})']\n",
    "                    \n",
    "                    # Check for NaNs and print information\n",
    "                    columns_to_check = {\n",
    "                        'propose_correct_answer': true_labels,\n",
    "                        f'Decision ({model_name})': decisions,\n",
    "                        f'External Confidence ({model_name})': external_confidence,\n",
    "                        f'Internal Confidence ({model_name})': internal_confidence\n",
    "                    }\n",
    "                else:\n",
    "                    true_labels = df['propose_correct_answer']\n",
    "                    decisions = df[f'Decision (temp {temperature}) ({model_name})']\n",
    "                    external_confidence = df[f'External Confidence (temp {temperature}) ({model_name})']\n",
    "                    internal_confidence = df[f'Internal Confidence (temp {temperature}) ({model_name})']\n",
    "                    \n",
    "                    # Check for NaNs and print information\n",
    "                    columns_to_check = {\n",
    "                        'propose_correct_answer': true_labels,\n",
    "                        f'Decision (temp {temperature}) ({model_name})': decisions,\n",
    "                        f'External Confidence (temp {temperature}) ({model_name})': external_confidence,\n",
    "                        f'Internal Confidence (temp {temperature}) ({model_name})': internal_confidence\n",
    "                    }\n",
    "            else:\n",
    "                if temperature is None:\n",
    "                    true_labels = df[f'propose_correct_answer ({target_ratio*100:.0f}%)']\n",
    "                    decisions = df[f'Decision ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    external_confidence = df[f'External Confidence ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    internal_confidence = df[f'Internal Confidence ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    \n",
    "                    # Check for NaNs and print information\n",
    "                    columns_to_check = {\n",
    "                        f'propose_correct_answer ({target_ratio*100:.0f}%)': true_labels,\n",
    "                        f'Decision ({target_ratio*100:.0f}%) ({model_name})': decisions,\n",
    "                        f'External Confidence ({target_ratio*100:.0f}%) ({model_name})': external_confidence,\n",
    "                        f'Internal Confidence ({target_ratio*100:.0f}%) ({model_name})': internal_confidence\n",
    "                    }\n",
    "                else:\n",
    "                    true_labels = df[f'propose_correct_answer ({target_ratio*100:.0f}%)']\n",
    "                    decisions = df[f'Decision (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    external_confidence = df[f'External Confidence (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    internal_confidence = df[f'Internal Confidence (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    \n",
    "                    # Check for NaNs and print information\n",
    "                    columns_to_check = {\n",
    "                        f'propose_correct_answer ({target_ratio*100:.0f}%)': true_labels,\n",
    "                        f'Decision (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})': decisions,\n",
    "                        f'External Confidence (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})': external_confidence,\n",
    "                        f'Internal Confidence (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})': internal_confidence\n",
    "                    }\n",
    "        elif prompt_id == 2:\n",
    "            if target_ratio is None:\n",
    "                if temperature is None:\n",
    "                    true_labels = df['correct_choice_id']\n",
    "                    decisions = df[f'Decision (AB) ({model_name})']\n",
    "                    external_confidence = df[f'External Confidence (AB) ({model_name})']\n",
    "                    internal_confidence = df[f'Internal Confidence (AB) ({model_name})']\n",
    "                    \n",
    "                    # Check for NaNs and print information\n",
    "                    columns_to_check = {\n",
    "                        'correct_choice_id': true_labels,\n",
    "                        f'Decision (AB) ({model_name})': decisions,\n",
    "                        f'External Confidence (AB) ({model_name})': external_confidence,\n",
    "                        f'Internal Confidence (AB) ({model_name})': internal_confidence\n",
    "                    }\n",
    "                else:\n",
    "                    true_labels = df['correct_choice_id']\n",
    "                    decisions = df[f'Decision (AB) (temp {temperature}) ({model_name})']\n",
    "                    external_confidence = df[f'External Confidence (AB) (temp {temperature}) ({model_name})']\n",
    "                    internal_confidence = df[f'Internal Confidence (AB) (temp {temperature}) ({model_name})']\n",
    "                    \n",
    "                    # Check for NaNs and print information\n",
    "                    columns_to_check = {\n",
    "                        'correct_choice_id': true_labels,\n",
    "                        f'Decision (AB) (temp {temperature}) ({model_name})': decisions,\n",
    "                        f'External Confidence (AB) (temp {temperature}) ({model_name})': external_confidence,\n",
    "                        f'Internal Confidence (AB) (temp {temperature}) ({model_name})': internal_confidence\n",
    "                    }\n",
    "            else:\n",
    "                if temperature is None:\n",
    "                    true_labels = df[f'correct_choice_id ({target_ratio*100:.0f}%)']\n",
    "                    decisions = df[f'Decision (AB) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    external_confidence = df[f'External Confidence (AB) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    internal_confidence = df[f'Internal Confidence (AB) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    \n",
    "                    # Check for NaNs and print information\n",
    "                    columns_to_check = {\n",
    "                        f'correct_choice_id ({target_ratio*100:.0f}%)': true_labels,\n",
    "                        f'Decision (AB) ({target_ratio*100:.0f}%) ({model_name})': decisions,\n",
    "                        f'External Confidence (AB) ({target_ratio*100:.0f}%) ({model_name})': external_confidence,\n",
    "                        f'Internal Confidence (AB) ({target_ratio*100:.0f}%) ({model_name})': internal_confidence\n",
    "                    }\n",
    "                else:\n",
    "                    true_labels = df[f'correct_choice_id ({target_ratio*100:.0f}%)']\n",
    "                    decisions = df[f'Decision (AB) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    external_confidence = df[f'External Confidence (AB) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    internal_confidence = df[f'Internal Confidence (AB) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    \n",
    "                    # Check for NaNs and print information\n",
    "                    columns_to_check = {\n",
    "                        f'correct_choice_id ({target_ratio*100:.0f}%)': true_labels,\n",
    "                        f'Decision (AB) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})': decisions,\n",
    "                        f'External Confidence (AB) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})': external_confidence,\n",
    "                        f'Internal Confidence (AB) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})': internal_confidence\n",
    "                    }\n",
    "            \n",
    "        elif prompt_id == 3:\n",
    "            if target_ratio is None:\n",
    "                if temperature is None:\n",
    "                    true_labels = df['correct_choice_id']\n",
    "                    decisions = df[f'Decision (AB2) ({model_name})']\n",
    "                    external_confidence = df[f'External Confidence (AB2) ({model_name})']\n",
    "                    internal_confidence = df[f'Internal Confidence (AB2) ({model_name})']\n",
    "                    \n",
    "                    # Check for NaNs and print information\n",
    "                    columns_to_check = {\n",
    "                        'correct_choice_id': true_labels,\n",
    "                        f'Decision (AB2) ({model_name})': decisions,\n",
    "                        f'External Confidence (AB2) ({model_name})': external_confidence,\n",
    "                        f'Internal Confidence (AB2) ({model_name})': internal_confidence\n",
    "                    }\n",
    "                else:\n",
    "                    true_labels = df['correct_choice_id']\n",
    "                    decisions = df[f'Decision (AB2) (temp {temperature}) ({model_name})']\n",
    "                    external_confidence = df[f'External Confidence (AB2) (temp {temperature}) ({model_name})']\n",
    "                    internal_confidence = df[f'Internal Confidence (AB2) (temp {temperature}) ({model_name})']\n",
    "                    \n",
    "                    # Check for NaNs and print information\n",
    "                    columns_to_check = {\n",
    "                        'correct_choice_id': true_labels,\n",
    "                        f'Decision (AB2) (temp {temperature}) ({model_name})': decisions,\n",
    "                        f'External Confidence (AB2) (temp {temperature}) ({model_name})': external_confidence,\n",
    "                        f'Internal Confidence (AB2) (temp {temperature}) ({model_name})': internal_confidence\n",
    "                    }\n",
    "            else:\n",
    "                if temperature is None:\n",
    "                    true_labels = df[f'correct_choice_id ({target_ratio*100:.0f}%)']\n",
    "                    decisions = df[f'Decision (AB2) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    external_confidence = df[f'External Confidence (AB2) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    internal_confidence = df[f'Internal Confidence (AB2) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    \n",
    "                    # Check for NaNs and print information\n",
    "                    columns_to_check = {\n",
    "                        f'correct_choice_id ({target_ratio*100:.0f}%)': true_labels,\n",
    "                        f'Decision (AB2) ({target_ratio*100:.0f}%) ({model_name})': decisions,\n",
    "                        f'External Confidence (AB2) ({target_ratio*100:.0f}%) ({model_name})': external_confidence,\n",
    "                        f'Internal Confidence (AB2) ({target_ratio*100:.0f}%) ({model_name})': internal_confidence\n",
    "                    }\n",
    "                else:\n",
    "                    true_labels = df[f'correct_choice_id ({target_ratio*100:.0f}%)']\n",
    "                    decisions = df[f'Decision (AB2) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    external_confidence = df[f'External Confidence (AB2) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    internal_confidence = df[f'Internal Confidence (AB2) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    \n",
    "                    # Check for NaNs and print information\n",
    "                    columns_to_check = {\n",
    "                        f'correct_choice_id ({target_ratio*100:.0f}%)': true_labels,\n",
    "                        f'Decision (AB2) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})': decisions,\n",
    "                        f'External Confidence (AB2) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})': external_confidence,\n",
    "                        f'Internal Confidence (AB2) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})': internal_confidence\n",
    "                    }\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"The prompt_id must be either 1 or 2 or 3\")\n",
    "            \n",
    "        nan_indices = set()\n",
    "        for col_name, col_data in columns_to_check.items():\n",
    "            nans = col_data.isna()\n",
    "            total_nans = nans.sum()\n",
    "            if total_nans > 0:\n",
    "                col_nan_indices = df[nans].index.tolist()\n",
    "                nan_indices.update(col_nan_indices)\n",
    "                print(f\"Column '{col_name}' has {total_nans} NaN(s).\")\n",
    "                print(f\"Row indices with NaNs: {col_nan_indices}\")\n",
    "        \n",
    "        # Remove rows with NaNs\n",
    "        nan_indices = list(nan_indices)\n",
    "        df_cleaned = df.drop(index=nan_indices)\n",
    "        \n",
    "        # Re-extract the true labels, predicted labels, and predicted probabilities after removing NaNs\n",
    "        if prompt_id == 1:\n",
    "            if target_ratio is None:\n",
    "                if temperature is None:\n",
    "                    true_labels_cleaned = df_cleaned['propose_correct_answer']\n",
    "                    decisions_cleaned = df_cleaned[f'Decision ({model_name})']\n",
    "                    external_confidence_cleaned = df_cleaned[f'External Confidence ({model_name})']\n",
    "                    internal_confidence_cleaned = df_cleaned[f'Internal Confidence ({model_name})']\n",
    "                else:\n",
    "                    true_labels_cleaned = df_cleaned['propose_correct_answer']\n",
    "                    decisions_cleaned = df_cleaned[f'Decision (temp {temperature}) ({model_name})']\n",
    "                    external_confidence_cleaned = df_cleaned[f'External Confidence (temp {temperature}) ({model_name})']\n",
    "                    internal_confidence_cleaned = df_cleaned[f'Internal Confidence (temp {temperature}) ({model_name})']\n",
    "            else:\n",
    "                if temperature is None:\n",
    "                    true_labels_cleaned = df_cleaned[f'propose_correct_answer ({target_ratio*100:.0f}%)']\n",
    "                    decisions_cleaned = df_cleaned[f'Decision ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    external_confidence_cleaned = df_cleaned[f'External Confidence ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    internal_confidence_cleaned = df_cleaned[f'Internal Confidence ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                else:\n",
    "                    true_labels_cleaned = df_cleaned[f'propose_correct_answer ({target_ratio*100:.0f}%)']\n",
    "                    decisions_cleaned = df_cleaned[f'Decision (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    external_confidence_cleaned = df_cleaned[f'External Confidence (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    internal_confidence_cleaned = df_cleaned[f'Internal Confidence (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "        elif prompt_id == 2:\n",
    "            if target_ratio is None:\n",
    "                if temperature is None:\n",
    "                    true_labels_cleaned = df_cleaned['correct_choice_id']\n",
    "                    decisions_cleaned = df_cleaned[f'Decision (AB) ({model_name})']\n",
    "                    external_confidence_cleaned = df_cleaned[f'External Confidence (AB) ({model_name})']\n",
    "                    internal_confidence_cleaned = df_cleaned[f'Internal Confidence (AB) ({model_name})']\n",
    "                else:\n",
    "                    true_labels_cleaned = df_cleaned['correct_choice_id']\n",
    "                    decisions_cleaned = df_cleaned[f'Decision (AB) (temp {temperature}) ({model_name})']\n",
    "                    external_confidence_cleaned = df_cleaned[f'External Confidence (AB) (temp {temperature}) ({model_name})']\n",
    "                    internal_confidence_cleaned = df_cleaned[f'Internal Confidence (AB) (temp {temperature}) ({model_name})']\n",
    "            else:\n",
    "                if temperature is None:\n",
    "                    true_labels_cleaned = df_cleaned[f'correct_choice_id ({target_ratio*100:.0f}%)']\n",
    "                    decisions_cleaned = df_cleaned[f'Decision (AB) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    external_confidence_cleaned = df_cleaned[f'External Confidence (AB) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    internal_confidence_cleaned = df_cleaned[f'Internal Confidence (AB) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                else:\n",
    "                    true_labels_cleaned = df_cleaned[f'correct_choice_id ({target_ratio*100:.0f}%)']\n",
    "                    decisions_cleaned = df_cleaned[f'Decision (AB) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    external_confidence_cleaned = df_cleaned[f'External Confidence (AB) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    internal_confidence_cleaned = df_cleaned[f'Internal Confidence (AB) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "        elif prompt_id == 3:\n",
    "            if target_ratio is None:\n",
    "                if temperature is None:\n",
    "                    true_labels_cleaned = df_cleaned['correct_choice_id']\n",
    "                    decisions_cleaned = df_cleaned[f'Decision (AB2) ({model_name})']\n",
    "                    external_confidence_cleaned = df_cleaned[f'External Confidence (AB2) ({model_name})']\n",
    "                    internal_confidence_cleaned = df_cleaned[f'Internal Confidence (AB2) ({model_name})']\n",
    "                else:\n",
    "                    true_labels_cleaned = df_cleaned['correct_choice_id']\n",
    "                    decisions_cleaned = df_cleaned[f'Decision (AB2) (temp {temperature}) ({model_name})']\n",
    "                    external_confidence_cleaned = df_cleaned[f'External Confidence (AB2) (temp {temperature}) ({model_name})']\n",
    "                    internal_confidence_cleaned = df_cleaned[f'Internal Confidence (AB2) (temp {temperature}) ({model_name})']\n",
    "            else:\n",
    "                if temperature is None:\n",
    "                    true_labels_cleaned = df_cleaned[f'correct_choice_id ({target_ratio*100:.0f}%)']\n",
    "                    decisions_cleaned = df_cleaned[f'Decision (AB2) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    external_confidence_cleaned = df_cleaned[f'External Confidence (AB2) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    internal_confidence_cleaned = df_cleaned[f'Internal Confidence (AB2) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                else:\n",
    "                    true_labels_cleaned = df_cleaned[f'correct_choice_id ({target_ratio*100:.0f}%)']\n",
    "                    decisions_cleaned = df_cleaned[f'Decision (AB2) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    external_confidence_cleaned = df_cleaned[f'External Confidence (AB2) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "                    internal_confidence_cleaned = df_cleaned[f'Internal Confidence (AB2) (temp {temperature}) ({target_ratio*100:.0f}%) ({model_name})']\n",
    "        else:\n",
    "            raise ValueError(\"The prompt_id must be either 1 or 2 or 3\")\n",
    "\n",
    "        # Adjust the confidence values based on the decision\n",
    "        external_confidence_adjusted = np.where(decisions_cleaned == 0, external_confidence_cleaned * -1, external_confidence_cleaned)\n",
    "        internal_confidence_adjusted = np.where(decisions_cleaned == 0, internal_confidence_cleaned * -1, internal_confidence_cleaned)\n",
    "        \n",
    "        external_confidence_adjusted = (external_confidence_adjusted / 2) + 0.5\n",
    "        internal_confidence_adjusted = (internal_confidence_adjusted / 2) + 0.5\n",
    "\n",
    "        # Calculate ROC curve and AUROC\n",
    "        if type.upper() == \"ROC\":\n",
    "            a, b, _ = roc_curve(true_labels_cleaned, external_confidence_adjusted)\n",
    "            c = auc(a, b)\n",
    "            external_list.append(c)\n",
    "        \n",
    "            d, e, _ = roc_curve(true_labels_cleaned, internal_confidence_adjusted)\n",
    "            f = auc(d, e)\n",
    "            internal_list.append(f)\n",
    "            \n",
    "        elif type.upper() == \"PRC\":\n",
    "            b, a, _ = precision_recall_curve(true_labels_cleaned, external_confidence_adjusted)\n",
    "            c = auc(a, b)\n",
    "            external_list.append(c)\n",
    "            \n",
    "            e, d, _ = precision_recall_curve(true_labels_cleaned, internal_confidence_adjusted)\n",
    "            f = auc(d, e)\n",
    "            internal_list.append(f)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"The type must be either ROC or PRC\")\n",
    "        \n",
    "        # Plot ROC or PRC curve\n",
    "        ax = axes[i]\n",
    "        ax.plot(a, b, color='darkorange', lw=2, label=f'Explicit Probability (AU{type} = {c:.3f})')\n",
    "        ax.plot(d, e, color='blue', lw=2, label=f'Implicit Probability (AU{type} = {f:.3f})')\n",
    "        \n",
    "        # Increase the font size for labels, ticks, and title\n",
    "        label_fontsize = 14\n",
    "        title_fontsize = 12\n",
    "        tick_fontsize = 12\n",
    "        \n",
    "        if type.upper() == \"ROC\":\n",
    "            ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            ax.set_xlabel('False Positive Rate', fontsize=label_fontsize)\n",
    "            ax.set_ylabel('True Positive Rate', fontsize=label_fontsize)\n",
    "            ax.legend(loc=\"lower right\", fontsize=tick_fontsize)\n",
    "        elif type.upper() == \"PRC\":\n",
    "            ax.set_xlabel('Recall', fontsize=label_fontsize)\n",
    "            ax.set_ylabel('Precision', fontsize=label_fontsize)\n",
    "            ax.legend(loc=\"lower left\", fontsize=tick_fontsize)\n",
    "        else:\n",
    "            raise ValueError(\"The type must be either ROC or PRC\")\n",
    "        \n",
    "        # Set the title based on the csv_file and target_ratio\n",
    "        if csv_file.split(\".\")[0] == \"clinical_knowledge\":\n",
    "            title_str = \"MMLU-CK\"\n",
    "        elif csv_file.split(\".\")[0] == \"college_medicine\":\n",
    "            title_str = \"MMLU-CM\"\n",
    "        elif csv_file.split(\".\")[0] == \"medQA_en\":\n",
    "            title_str = \"USMLE\"\n",
    "        elif csv_file.split(\".\")[0] == \"medQA_zh\":\n",
    "            title_str = \"MCMLE\"\n",
    "        else:\n",
    "            title_str = \"MGB-SDoH\"\n",
    "        \n",
    "        ax.set_title(f'{model_name}', fontsize=title_fontsize) if target_ratio is None else ax.set_title(f'{model_name} ({target_ratio*100:.0f}% imbalance)', fontsize=title_fontsize)\n",
    "        \n",
    "        # Increase the font size of the tick labels\n",
    "        ax.tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
    "\n",
    "    # Adjust layout and save the plot as a PNG file\n",
    "    plt.tight_layout()\n",
    "    if prompt_id == 1:\n",
    "        if target_ratio is None:\n",
    "            output_file_name = f\"{csv_file.split('.')[0]}_{type}_All_Models.png\"\n",
    "        else:\n",
    "            output_file_name = f\"{csv_file.split('.')[0]}_{type}_All_Models ({target_ratio*100:.0f} imbalance).png\"\n",
    "    elif prompt_id == 2:\n",
    "        if target_ratio is None:\n",
    "            output_file_name = f\"{csv_file.split('.')[0]}_{type}_All_Models (AB).png\"\n",
    "        else:\n",
    "            output_file_name = f\"{csv_file.split('.')[0]}_{type}_All_Models (AB) ({target_ratio*100:.0f} imbalance).png\"\n",
    "    elif prompt_id == 3:\n",
    "        if target_ratio is None:\n",
    "            output_file_name = f\"{csv_file.split('.')[0]}_{type}_All_Models (AB2).png\"\n",
    "        else:\n",
    "            output_file_name = f\"{csv_file.split('.')[0]}_{type}_All_Models (AB2) ({target_ratio*100:.0f} imbalance).png\"\n",
    "    else:\n",
    "        raise ValueError(\"The prompt_id must be either 1 or 2 or 3\")\n",
    "    plt.savefig(output_file_name)\n",
    "    plt.show()\n",
    "\n",
    "    return external_list, internal_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedb7b4-7586-43d5-a765-32ccf632d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_names = ['Qwen2-72B-Instruct', 'Qwen2-7B-Instruct', 'Meta-Llama-3.1-70B-Instruct', 'Meta-Llama-3.1-8B-Instruct', 'gemma-2-27b-it', 'gemma-2-9b-it', 'Mistral-Large-Instruct-2407', 'Mistral-7B-Instruct-v0.3', 'Yi-1.5-34B-Chat', 'Yi-1.5-9B-Chat', 'Phi-3-medium-128k-instruct', 'Phi-3-mini-128k-instruct']\n",
    "large_model_names = ['Qwen2-72B-Instruct', 'Meta-Llama-3.1-70B-Instruct', 'gemma-2-27b-it', 'Mistral-Large-Instruct-2407', 'Yi-1.5-34B-Chat', 'Phi-3-medium-128k-instruct']  # List of model names to analyze\n",
    "first_half_model_names = ['Qwen2-72B-Instruct', 'Qwen2-7B-Instruct', 'Meta-Llama-3.1-70B-Instruct', 'Meta-Llama-3.1-8B-Instruct', 'gemma-2-27b-it', 'gemma-2-9b-it']  # List of model names to analyze\n",
    "second_half_model_names = ['Mistral-Large-Instruct-2407', 'Mistral-7B-Instruct-v0.3', 'Yi-1.5-34B-Chat', 'Yi-1.5-9B-Chat', 'Phi-3-medium-128k-instruct', 'Phi-3-mini-128k-instruct']  # List of model names to analyze\n",
    "small_model_names = ['Qwen2-7B-Instruct', 'Meta-Llama-3.1-8B-Instruct', 'gemma-2-9b-it', 'Mistral-7B-Instruct-v0.3', 'Yi-1.5-9B-Chat', 'Phi-3-mini-128k-instruct']\n",
    "llama_model_names = ['Meta-Llama-3.1-70B-Instruct', 'Meta-Llama-3.1-8B-Instruct']\n",
    "mistral_model_names = ['Mistral-Large-Instruct-2407', 'Mistral-7B-Instruct-v0.3']\n",
    "fine_tuned_models = ['Meta-Llama-3.1-8B-Instruct', 'Meta-Llama-3.1-8B-Instruct (LoRA 8)', 'Meta-Llama-3.1-8B-Instruct (LoRA 9)', 'Meta-Llama-3.1-8B-Instruct (LoRA 10)']\n",
    "type = \"PRC\"\n",
    "csv_file = '../data/SDoH.csv'  # Path to your CSV file\n",
    "prompt_id = 2\n",
    "target_ratio = None\n",
    "temperature = None\n",
    "external_list, internal_list = calculate_and_plot(type, csv_file, large_model_names, prompt_id, target_ratio, temperature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
