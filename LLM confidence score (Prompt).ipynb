{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e0d1bc-4cf2-4de8-9bb1-2c9b7b514c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2189abac-6376-4cd3-baf7-9576bb3d671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_raw_and_answer_and_confidence(raw_response, generated_tokens, transition_scores):\n",
    "    #print(f\"Model raw response is:\\n{raw_response}\")\n",
    "    # Regular expression patterns to extract the decision and percentage\n",
    "    decision_pattern = r'Correctness:\\s*(\\w+)'\n",
    "    external_confidence_pattern = r'Confidence:\\s*(\\d+)%?'\n",
    "    \n",
    "    # Extracting the decision\n",
    "    decision_match = re.search(decision_pattern, raw_response)\n",
    "    decision = decision_match.group(1) if decision_match else None\n",
    "    \n",
    "    # Extracting the confidecne\n",
    "    external_confidence_match = re.search(external_confidence_pattern, raw_response)\n",
    "    external_confidence = external_confidence_match.group(1) if external_confidence_match else None\n",
    "\n",
    "    assert decision is not None and external_confidence is not None, \"Cannot extract decision or external external_confidence\"\n",
    "    assert decision == 'Yes' or decision == 'No', \"The decision must be 'Yes' or 'No'\"\n",
    "    try:\n",
    "        external_confidence = int(external_confidence) / 100.0\n",
    "    except:\n",
    "        raise ValueError(f\"The external confidence {external_confidence} is not a number\")\n",
    "    assert 0 <= external_confidence <= 1, \"External confidence must be between 0 and 1\"\n",
    "    decision = 1 if decision == 'Yes' else 0\n",
    "\n",
    "    for tok, score in zip(generated_tokens, transition_scores):\n",
    "        score_cpu = score.cpu().numpy()\n",
    "        #print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score_cpu:.3f} | {np.exp(score_cpu):.2%}\")\n",
    "        if \"Yes\" in tokenizer.decode(tok) or \"No\" in tokenizer.decode(tok):\n",
    "            internal_confidence = np.exp(score_cpu)\n",
    "            break\n",
    "    \n",
    "    return raw_response, decision, external_confidence, internal_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939ae0b6-5482-4daa-b075-3cda13dba540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_raw_and_answer_and_confidence_A_B(raw_response, generated_tokens, transition_scores):\n",
    "    #print(f\"Model raw response is:\\n{raw_response}\")\n",
    "    # Regular expression patterns to extract the decision and percentage\n",
    "    decision_pattern = r'Answer:\\s*(\\w)'\n",
    "    external_confidence_pattern = r'Probability:\\s*(\\d+)%?'\n",
    "    \n",
    "    # Extracting the decision\n",
    "    decision_match = re.search(decision_pattern, raw_response)\n",
    "    decision = decision_match.group(1) if decision_match else None\n",
    "    \n",
    "    # Extracting the confidecne\n",
    "    external_confidence_match = re.search(external_confidence_pattern, raw_response)\n",
    "    external_confidence = external_confidence_match.group(1) if external_confidence_match else None\n",
    "\n",
    "    assert decision is not None and external_confidence is not None, \"Cannot extract decision or external confidence\"\n",
    "    assert decision == 'A' or decision == 'B', \"The decision must be 'A' or 'B'\"\n",
    "    try:\n",
    "        external_confidence = int(external_confidence) / 100.0\n",
    "    except:\n",
    "        raise ValueError(f\"The external confidence {external_confidence} is not a number\")\n",
    "    assert 0 <= external_confidence <= 1, \"External confidence must be between 0 and 1\"\n",
    "    decision = 1 if decision == 'A' else 0\n",
    "\n",
    "    for tok, score in zip(generated_tokens, transition_scores):\n",
    "        score_cpu = score.cpu().numpy()\n",
    "        #print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score_cpu:.3f} | {np.exp(score_cpu):.2%}\")\n",
    "        if tokenizer.decode(tok).strip() == 'A' or tokenizer.decode(tok).strip() == 'B':\n",
    "            internal_confidence = np.exp(score_cpu)\n",
    "            break\n",
    "\n",
    "    return raw_response, decision, external_confidence, internal_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af030c3a-3bf3-444c-9172-afb347ce779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(filename, model_name, temperature):\n",
    "    df = pd.read_csv(filename)\n",
    "    question_list = df['question'].tolist()\n",
    "    answer_list = df['proposed_answer'].tolist()\n",
    "\n",
    "    input_text = '''You are an experienced doctor who has rich clinical and medical knowledge. Now, given the following question and the proposed answer from one of your students.\n",
    "    Question: [QUESTION]\n",
    "    Stuent Proposed Answer: [ANSWER]\n",
    "    Please decide if the student's answer is correct or not and rate the confidence of your decision as a percentage ranging from 0% to 100% (inclusive), where 0% is the least confident and 100% is the most confident. Then provide your reasoning of your decision.\n",
    "    Return your response as follows:\n",
    "    Correctness: Yes/No\n",
    "    Confidence: [PERCENTAGE]\n",
    "    Explanation: [EXPLANATION]\n",
    "    \n",
    "    Your Response:'''\n",
    "    actual_input_text_list = []\n",
    "    for question, answer in zip(question_list, answer_list):\n",
    "        actual_input_text = input_text.replace('[QUESTION]', question).replace('[ANSWER]', answer)\n",
    "        actual_input_text_list.append(actual_input_text)\n",
    "\n",
    "    raw_response_list = []\n",
    "    decision_list = []\n",
    "    external_confidence_list = []\n",
    "    internal_confidence_list = []\n",
    "    fails_count = 0\n",
    "    for i, actual_input_text in enumerate(actual_input_text_list):\n",
    "        print(f\"Processing prompt {i + 1}/{len(actual_input_text_list)}\")\n",
    "        input_ids = tokenizer(actual_input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**input_ids, max_new_tokens = 64, temperature = temperature, do_sample = temperature > 0, return_dict_in_generate=True, output_scores=True)\n",
    "        \n",
    "        transition_scores = model.compute_transition_scores(\n",
    "            outputs.sequences, outputs.scores, normalize_logits=True\n",
    "        )\n",
    "\n",
    "        input_length = input_ids.input_ids.shape[1]\n",
    "        generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "        response = tokenizer.decode(outputs.sequences[:, input_length:][0], skip_special_tokens=True)\n",
    "        try:\n",
    "            raw_response, decision, external_confidence, internal_confidence = extra_raw_and_answer_and_confidence(response, generated_tokens[0], transition_scores[0])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            fails_count += 1\n",
    "            print(f\"Cannot extract the decision or the confidence. Current fails: {fails_count}/{i + 1}\")\n",
    "            raw_response, decision, external_confidence, internal_confidence = response.split(\"\\n\", 1)[-1], np.nan, np.nan, np.nan\n",
    "        raw_response_list.append(raw_response)\n",
    "        decision_list.append(decision)\n",
    "        external_confidence_list.append(external_confidence)\n",
    "        internal_confidence_list.append(internal_confidence)\n",
    "        print(f\"The decision is {decision}\")\n",
    "        print(f\"The external confidence is {external_confidence}\")\n",
    "        print(f\"The internal confidence is {internal_confidence}\")\n",
    "    return raw_response_list, decision_list, external_confidence_list, internal_confidence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd8690-0979-4a4e-aab9-9a81fdf9a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_A_B(filename, model_name, prompt_id, target_ratio, temperature):\n",
    "    df = pd.read_csv(filename)\n",
    "    question_list = df['question'].tolist()\n",
    "    if target_ratio is None:\n",
    "        choice_A_str_list = df['choice_A_str'].tolist()\n",
    "        choice_B_str_list = df['choice_B_str'].tolist()\n",
    "    else:\n",
    "        choice_A_str_list = df[f'choice_A_str ({target_ratio*100:.0f}%)'].tolist()\n",
    "        choice_B_str_list = df[f'choice_B_str ({target_ratio*100:.0f}%)'].tolist()\n",
    "\n",
    "    if prompt_id == 2:\n",
    "        input_text = '''You are an experienced doctor who has rich clinical and medical knowledge. Now, given the following question and the corresponding options:\n",
    "        Question: [QUESTION]\n",
    "        Options:  A.[Option A]; B.[Option B]\n",
    "        Please choose the correct option and rate the probability of the option being correct as a percentage ranging from 0% to 100% (inclusive), where 0% is the least probability and 100% is the highest probability for the option to be correct. Then provide your reasoning of your decision.\n",
    "        Return your response as follows:\n",
    "        Answer: A/B\n",
    "        Probability: [PERCENTAGE]\n",
    "        Explanation: [EXPLANATION]\n",
    "        \n",
    "        Your Response:'''\n",
    "\n",
    "    elif prompt_id == 3:\n",
    "        input_text = '''You are an experienced doctor who has rich clinical and medical knowledge. Please choose the correct option and determine the probability of the selection for the following question:\n",
    "        Question: [QUESTION]\n",
    "        Options:  A.[Option A]; B.[Option B]\n",
    "        The probability should be listed in percentage format ranging from 0% to 100%. \n",
    "        Please return your response as follows:\n",
    "        Answer: A/B\n",
    "        Probability: [PERCENTAGE]\n",
    "        \n",
    "        Your Response:'''\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"The prompt_id must be either 2 or 3\")\n",
    "        \n",
    "    actual_input_text_list = []\n",
    "    for question, choice_A_str, choice_B_str in zip(question_list, choice_A_str_list, choice_B_str_list):\n",
    "        actual_input_text = input_text.replace('[QUESTION]', question).replace('[Option A]', choice_A_str).replace('[Option B]', choice_B_str)\n",
    "        actual_input_text_list.append(actual_input_text)\n",
    "\n",
    "    raw_response_list = []\n",
    "    decision_list = []\n",
    "    external_confidence_list = []\n",
    "    internal_confidence_list = []\n",
    "    fails_count = 0\n",
    "    for i, actual_input_text in enumerate(actual_input_text_list):\n",
    "        print(f\"Processing prompt {i + 1}/{len(actual_input_text_list)}\")\n",
    "        #print(f\"The input text is:\\n{actual_input_text}\")\n",
    "        input_ids = tokenizer(actual_input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**input_ids, max_new_tokens = 64, temperature = temperature, do_sample = temperature > 0, return_dict_in_generate=True, output_scores=True)\n",
    "        \n",
    "        transition_scores = model.compute_transition_scores(\n",
    "            outputs.sequences, outputs.scores, normalize_logits=True\n",
    "        )\n",
    "\n",
    "        input_length = input_ids.input_ids.shape[1]\n",
    "        generated_tokens = outputs.sequences[:, input_length:]\n",
    "\n",
    "        response = tokenizer.decode(outputs.sequences[:, input_length:][0], skip_special_tokens=True)\n",
    "        try:\n",
    "            raw_response, decision, external_confidence, internal_confidence = extra_raw_and_answer_and_confidence_A_B(response, generated_tokens[0], transition_scores[0])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            fails_count += 1\n",
    "            print(f\"Cannot extract the decision or the confidence. Current fails: {fails_count}/{i + 1}\")\n",
    "            raw_response, decision, external_confidence, internal_confidence = response.split(\"\\n\", 1)[-1], np.nan, np.nan, np.nan\n",
    "        raw_response_list.append(raw_response)\n",
    "        decision_list.append(decision)\n",
    "        external_confidence_list.append(external_confidence)\n",
    "        internal_confidence_list.append(internal_confidence)\n",
    "        print(f\"The decision is {decision}\")\n",
    "        print(f\"The external confidence is {external_confidence}\")\n",
    "        print(f\"The internal confidence is {internal_confidence}\")\n",
    "    return raw_response_list, decision_list, external_confidence_list, internal_confidence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a824152-3f44-4cc1-ad7b-0bdf57b7f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"YOUR MODEL NAME\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586049f0-bf4d-4d04-be78-bd035a9cfc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model_name, filename_list, prompt_id_list, target_ratio_list, temperature_list):\n",
    "    for filename in filename_list:\n",
    "        for prompt_id in prompt_id_list:\n",
    "            for target_ratio in target_ratio_list:\n",
    "                for temperature in temperature_list:\n",
    "                    print(f\"Current parameters: model_name = {model_name}, filename = {filename}, prompt_id = {prompt_id}, target_ratio = {target_ratio}, temperature = {temperature}\")\n",
    "                    if prompt_id == 1:\n",
    "                        raw_response_list, decision_list, external_confidence_list, internal_confidence_list = process(filename, model_name, temperature)\n",
    "                        if target_ratio is None:\n",
    "                            df = pd.read_csv(filename)\n",
    "                            df[f'Raw Response (temp {temperature}) (temp {temperature}) ({model_name})'] = raw_response_list\n",
    "                            df[f'Decision (temp {temperature}) ({model_name})'] = decision_list\n",
    "                            df[f'External Confidence (temp {temperature}) ({model_name})'] = external_confidence_list\n",
    "                            df[f'Internal Confidence (temp {temperature}) ({model_name})'] = internal_confidence_list\n",
    "                            df.to_csv(filename, index = False)\n",
    "                            print(f\"Processed file saved to {filename}\")\n",
    "                        else:\n",
    "                            df = pd.read_csv(filename)\n",
    "                            df[f'Raw Response ({target_ratio*100:.0f}%) (temp {temperature}) ({model_name})'] = raw_response_list\n",
    "                            df[f'Decision ({target_ratio*100:.0f}%) (temp {temperature}) ({model_name})'] = decision_list\n",
    "                            df[f'External Confidence ({target_ratio*100:.0f}%) (temp {temperature}) ({model_name})'] = external_confidence_list\n",
    "                            df[f'Internal Confidence ({target_ratio*100:.0f}%) (temp {temperature}) ({model_name})'] = internal_confidence_list\n",
    "                            df.to_csv(filename, index = False)\n",
    "                            print(f\"Processed file saved to {filename}\")\n",
    "                    elif prompt_id == 2:\n",
    "                        raw_response_list, decision_list, external_confidence_list, internal_confidence_list = process_A_B(filename, model_name, prompt_id, target_ratio, temperature)\n",
    "                        if target_ratio is None:\n",
    "                            df = pd.read_csv(filename)\n",
    "                            df[f'Raw Response (AB) (temp {temperature}) ({model_name})'] = raw_response_list\n",
    "                            df[f'Decision (AB) (temp {temperature}) ({model_name})'] = decision_list\n",
    "                            df[f'External Confidence (AB) (temp {temperature}) ({model_name})'] = external_confidence_list\n",
    "                            df[f'Internal Confidence (AB) (temp {temperature}) ({model_name})'] = internal_confidence_list\n",
    "                            df.to_csv(filename, index = False)\n",
    "                            print(f\"Processed file saved to {filename}\")\n",
    "                        else:\n",
    "                            df = pd.read_csv(filename)\n",
    "                            df[f'Raw Response (AB) ({target_ratio*100:.0f}%) (temp {temperature}) ({model_name})'] = raw_response_list\n",
    "                            df[f'Decision (AB) ({target_ratio*100:.0f}%) (temp {temperature}) ({model_name})'] = decision_list\n",
    "                            df[f'External Confidence (AB) ({target_ratio*100:.0f}%) (temp {temperature}) ({model_name})'] = external_confidence_list\n",
    "                            df[f'Internal Confidence (AB) ({target_ratio*100:.0f}%) (temp {temperature}) ({model_name})'] = internal_confidence_list\n",
    "                            df.to_csv(filename, index = False)\n",
    "                            print(f\"Processed file saved to {filename}\")\n",
    "                    elif prompt_id == 3:\n",
    "                        raw_response_list, decision_list, external_confidence_list, internal_confidence_list = process_A_B(filename, model_name, prompt_id, target_ratio, temperature)\n",
    "                        if target_ratio is None:\n",
    "                            df = pd.read_csv(filename)\n",
    "                            df[f'Raw Response (AB2) (temp {temperature}) ({model_name})'] = raw_response_list\n",
    "                            df[f'Decision (AB2) (temp {temperature}) ({model_name})'] = decision_list\n",
    "                            df[f'External Confidence (AB2) (temp {temperature}) ({model_name})'] = external_confidence_list\n",
    "                            df[f'Internal Confidence (AB2) (temp {temperature}) ({model_name})'] = internal_confidence_list\n",
    "                            df.to_csv(filename, index = False)\n",
    "                            print(f\"Processed file saved to {filename}\")\n",
    "                        else:\n",
    "                            df = pd.read_csv(filename)\n",
    "                            df[f'Raw Response (AB2) ({target_ratio*100:.0f}%) (temp {temperature}) ({model_name})'] = raw_response_list\n",
    "                            df[f'Decision (AB2) ({target_ratio*100:.0f}%) (temp {temperature}) ({model_name})'] = decision_list\n",
    "                            df[f'External Confidence (AB2) ({target_ratio*100:.0f}%) (temp {temperature}) ({model_name})'] = external_confidence_list\n",
    "                            df[f'Internal Confidence (AB2) ({target_ratio*100:.0f}%) (temp {temperature}) ({model_name})'] = internal_confidence_list\n",
    "                            df.to_csv(filename, index = False)\n",
    "                            print(f\"Processed file saved to {filename}\")\n",
    "                    else:\n",
    "                        raise ValueError(\"The prompt_id must be 1 or 2 or 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f721f8-cc97-45c6-b369-daffcd547129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_task(task):\n",
    "    if task == 1:\n",
    "        # Task 1 (Main conclusion)\n",
    "        #filename_list = [\"clinical_knowledge.csv\", \"college_medicine.csv\", \"medQA_en.csv\", \"medQA_zh.csv\", \"SDoH.csv\"]\n",
    "        filename_list = [\"SDoH.csv\"]\n",
    "        prompt_id_list = [2]\n",
    "        target_ratio_list = [None]\n",
    "        temperature_list = [0.3, 0.7]\n",
    "        run(model_name, filename_list, prompt_id_list, target_ratio_list, temperature_list)\n",
    "\n",
    "    if task == 2:\n",
    "        # Task 2 (Imbalanced dataset)\n",
    "        filename_list = [\"medQA_en.csv\"]\n",
    "        prompt_id_list = [2]\n",
    "        target_ratio_list = [0.05, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        temperature_list = [0]\n",
    "        run(model_name, filename_list, prompt_id_list, target_ratio_list, temperature_list)\n",
    "\n",
    "    if task == 3:\n",
    "        # Task 3 (Sensitivity analysis)\n",
    "        filename_list = [\"medQA_en.csv\"]\n",
    "        prompt_id_list = [1, 3]\n",
    "        target_ratio_list = [None]\n",
    "        temperature_list = [0]\n",
    "        run(model_name, filename_list, prompt_id_list, target_ratio_list, temperature_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fe14d20-efcf-4563-b6a2-9c24fb232cbb",
   "metadata": {},
   "source": [
    "task_list = [1]\n",
    "for task in task_list:\n",
    "    start_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240035d7-dc14-4f25-9ac4-7ea15614b7d3",
   "metadata": {},
   "source": [
    "## Customized Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da946310-d292-4003-a138-5c544d4ff353",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = [\"anatomy.csv\", \"college_biology.csv\", \"medical_genetics.csv\", \"professional_medicine.csv\"]\n",
    "prompt_id_list = [2]\n",
    "target_ratio_list = [None]\n",
    "temperature_list = [0]\n",
    "run(model_name, filename_list, prompt_id_list, target_ratio_list, temperature_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
